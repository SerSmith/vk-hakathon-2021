{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "VK_cup_final_2021_submit.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "#ссылка на тестовые данные\n",
        "test_data_link = \"https://storage.yandexcloud.net/vk-cup-data/test_pipeline_questions.csv\""
      ],
      "outputs": [],
      "metadata": {
        "id": "6SZxnMubj9Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Большую часть времени работал над поиском по дампам википедии, но не успел:(\n",
        "\n",
        "Предлагается в бейзлайне заменять вопросительное слово на ответ, что получить из вопросов утверждения c более коректной постановкой слова, что в теории должно помогать находить более корректные вероятности.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Скачивание данных и библиотек"
      ],
      "metadata": {
        "id": "7GKqInuM9vsz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install transformers\n",
        "!pip install wget\n",
        "\n",
        "import wget\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "if not os.path.isfile('test_pipeline_questions.csv'):\n",
        "  wget.download(test_data_link)\n",
        "\n",
        "if not os.path.isfile('2021 VK Cup ML Finals.zip'):\n",
        "  wget.download(\"https://www.dropbox.com/sh/ds8wh42kftccd6f/AAAOtEW26zOauBc7KOx-FY4ba?dl=1\")\n",
        "  with zipfile.ZipFile('2021 VK Cup ML Finals.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.2.4-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 31.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.0)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.2.4\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 32.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=3d83d0c65cb6b69515b8525f91af03434f93ede258497aa8c2b5c443f5ae1b18\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-irvPhock-v",
        "outputId": "47a553cb-b48c-4f68-b283-d86d673aeae6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertTokenizer, BertForMaskedLM, RobertaForMaskedLM, RobertaTokenizer, RobertaForMaskedLM, RobertaTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "# colab уперся в лимит, поэтому приходитться использовать cpu\n",
        "GLOBAL_DEVICE = 'cpu'\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "pHWs7NS1b8_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Код общего назначения"
      ],
      "metadata": {
        "id": "B7B_9ut798EJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "class Data:\n",
        "  def __init__(self, test_size=0.25):\n",
        "    self.train_mode = None\n",
        "    self.train = None\n",
        "    self.test = None\n",
        "    self.test_size = test_size\n",
        "  \n",
        "  def __get_rus(self, seed):\n",
        "    rus_data = pd.read_csv(\"rus_train_dataset.csv\", delimiter='|')\n",
        "    rus_data = rus_data.rename(columns={'Unnamed: 0': 'index'})\n",
        "    if self.test_size != 1:\n",
        "      rus_train, rus_test = train_test_split(rus_data, random_state=seed, test_size=self.test_size)\n",
        "    else:\n",
        "      rus_test = rus_data\n",
        "      rus_train = rus_data.head(0)\n",
        "    return rus_train, rus_test\n",
        "\n",
        "  def __get_eng(self):\n",
        "    eng_data = pd.read_csv(\"en_to_rus_train_dataset.csv\", delimiter='|')\n",
        "    eng_data = eng_data.rename(columns={'Unnamed: 0': 'index'}) \n",
        "    return eng_data\n",
        "\n",
        "  def load_train_test(self, mode, seed):\n",
        "    print(f'Данные загружаютсяб режим {mode}')\n",
        "    rus_train, rus_test = self.__get_rus(seed)\n",
        "    if mode == \"RUS\":\n",
        "      self.train = rus_train\n",
        "      self.test = rus_test\n",
        "\n",
        "    elif mode == \"ALL\":\n",
        "      eng_train = self.__get_eng()\n",
        "      self.train = rus_train.append(eng_train, ignore_index=True)\n",
        "      self.test = rus_test\n",
        "    else:\n",
        "      ValueError('Wrong value for mode')\n",
        "\n",
        "      self.train['question'] = self.train['question'].replace('ё', 'е')\n",
        "      self.test['question'] = self.test['question'].replace('ё', 'е')\n",
        "    return None\n",
        "\n",
        "  def get_train(self, mode='RUS', seed=42):\n",
        "    if self.train_mode != mode:\n",
        "      self.load_train_test(mode, seed)\n",
        "    return self.train\n",
        "\n",
        "  def get_test(self, mode='RUS', seed=42):\n",
        "    if self.train_mode != mode:\n",
        "      self.load_train_test(mode, seed)\n",
        "    return self.test\n",
        "  \n",
        "\n",
        "\n",
        "def get_test_accuracy(data, model):\n",
        "  test = data.get_test()\n",
        "  my_prediction = model.predict(data)\n",
        "  merged = test.merge(my_prediction, on='index')\n",
        "  accuracy = (merged['right_answer_id'] == merged['my_prediction']).sum() / merged.shape[0]\n",
        "  return merged, accuracy\n",
        "\n",
        "def create_submission(questions_csv_path):\n",
        "  model = BaselineWordPos()\n",
        "  data = pd.read_csv('test_pipeline_questions.csv', encoding='utf-8', sep='|')\n",
        "  answers = [str(model.predict_one(data['question'][i], \n",
        "                     [data['ps_0'][i], data['ps_1'][i], data['ps_2'][i]])) for i in range(len(data))]\n",
        "  \n",
        "  with open('answer.txt', 'w') as f:\n",
        "    f.write('\\n'.join(answers))\n",
        "  return answers\n",
        "  "
      ],
      "outputs": [],
      "metadata": {
        "id": "34yGm_lPjW7T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class BaselineWordPos:\n",
        "  question_words = ['Кто',\n",
        "                    'Кого',\n",
        "                    'Кому',\n",
        "                    'Кем',\n",
        "                    'Что',\n",
        "                    'Ком',\n",
        "                    'Какой',\n",
        "                    'Какого',\n",
        "                    'Какому',\n",
        "                    'Каким',\n",
        "                    'Каком',\n",
        "                    'Какую',\n",
        "                    'Какая',\n",
        "                    'Какой',\n",
        "                    'Который',\n",
        "                    'Которого',\n",
        "                    'Которому',\n",
        "                    'Которым',\n",
        "                    'Где',\n",
        "                    'Когда',\n",
        "                    'Почему',\n",
        "                    'Зачем',\n",
        "                    'Куда',\n",
        "                    'Откуда',\n",
        "                    'Сколько',\n",
        "                    'Чей',\n",
        "                    'Как',\n",
        "                    'чем',\n",
        "                    'Какое',\n",
        "                    'Какие',\n",
        "                    'Скольким',\n",
        "                    'Скольких',\n",
        "                    'Столько',\n",
        "                    'Стольких',\n",
        "                    'Именно',\n",
        "                    'чем',\n",
        "                    'чего',\n",
        "                    'насколько',\n",
        "                    'каких',\n",
        "                    'какова',\n",
        "                    'Самый',\n",
        "                    'Самая',\n",
        "                    'Самое',\n",
        "                    'Какими'\n",
        "                    ]\n",
        "  question_words_lower = set([word.lower() for word in question_words])\n",
        "\n",
        "  def __init__(self):\n",
        "    model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
        "    self.tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "    self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(GLOBAL_DEVICE)\n",
        "  \n",
        "  def calc_likelihood(self, text):\n",
        "\n",
        "    tokens_ids = self.tokenizer.encode(text, return_tensors=\"pt\").to(GLOBAL_DEVICE)\n",
        "    prob = self.model(tokens_ids)\n",
        "\n",
        "    tokens = tokens_ids[0].cpu().tolist()\n",
        "    prob = prob[0].cpu()[0]\n",
        "\n",
        "    likelyhood = sum([prob[i, j].item() for i, j in enumerate(tokens)])\n",
        "    return likelyhood\n",
        "\n",
        "\n",
        "  def get_changed_question(self, question, answer):\n",
        "    changed_question = question\n",
        "\n",
        "    word_found = True\n",
        "\n",
        "    for word in self.question_words:\n",
        "      changed_question = \" \".join([answer if word_from_question.lower() == word.lower() else word_from_question for word_from_question in changed_question.split(' ')])\n",
        "\n",
        "    if changed_question == question:\n",
        "      changed_question = question + ' ' + answer\n",
        "\n",
        "      changed_question = \" \".join([word for word in changed_question.split(\" \") if word.lower() not in russian_stopwords])\n",
        "    return changed_question\n",
        "\n",
        "  def predict_one(self, question, answers):\n",
        "\n",
        "    scores = [self.calc_likelihood(self.get_changed_question(question, answer)) for answer in answers]\n",
        "    return np.argmax(scores)\n",
        "\n",
        "  def predict(self, data):\n",
        "    predict_data = data.get_test()\n",
        "    prediction = predict_data.progress_apply(lambda x: self.predict_one(x['question'], [x['ps_0'], x['ps_1'], x['ps_2']]), axis=1)\n",
        "    return pd.DataFrame({ 'index': predict_data['index'],\n",
        "                          'my_prediction': prediction})\n",
        "\n",
        "\n",
        "data = Data(test_size=1)\n",
        "model = BaselineWordPos()\n",
        "\n",
        "merged, accuracy = get_test_accuracy(data, model)\n",
        "print(accuracy)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZSHKes1FFc0S"
      }
    }
  ]
}